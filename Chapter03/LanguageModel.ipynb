{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch实现神经概率语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 赵振东"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本代码主要实现神经概率语言模型，使用了10000句微博用户文本数据。效果可能不会很好，主要是做个小实验演示一下，数据量有点小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 先看一下数据长什么样子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小马 也 疯狂 ------ 地位 之 争 。<br>\n",
    "那些 年 ， 我们 一起 偷看 过 的 电视 。 「 暴 走 漫画 」<br>\n",
    "北京 的 小 纯洁们 ， 周日见 。 # 硬汉 摆 拍 清纯 照 #<br>\n",
    "要是 这 一 年 哭泣 的 理由 不 再 是 难过 而 是 感动 会 多么 好<br>\n",
    "对于 国内 动漫 画作者 引用 工笔 素材 的 一些 个人 意见 。<br>\n",
    "猫咪 保镖 最 赞 了 ！ 你们 看懂 了 吗 ？ ！ （ 来自 ： 9gag ）<br>\n",
    "莫愁 和 所有 人 开 了 一 个 玩笑 —— 其实 ， 她 是 会 正常 唱歌 的 … …<br>\n",
    "你 见 过 皮卡丘 喝水 的 样子 吗 ？<br>\n",
    "如果 有 个 人 能 让 你 忘掉 过去 ， 那TA 很 可能 就是 你 的 未来 。<br>\n",
    "我 在 北京 ， 24 岁 ， 想 去 马尔代夫 ， 一 个 人 。<br>\n",
    "哥 你 还 跳 不 跳楼 了 ？ 我们 要 下班 啊 ！<br>\n",
    "龙 生 龙 ， 凤 生 凤 ， 是 个 喵咪 它 就萌 。<br>\n",
    "从 胚胎 期 开始 的 面部 特征 演变 过程<br>\n",
    "本 届 轮值 主席 王石致 开幕词 。 讲 60 岁 上 哈佛 。<br>\n",
    "非常 不 喜欢 北京 现在 的 天气 … … 非常 … …<br>\n",
    "我 第一 次 坐 飞机 是 进 安达 信 的 入职 培训 ， 在 深圳 。 你们 哪 ？<br>\n",
    "人生 如 戏 ， 全 靠 演技 。  小 受 吓坏 了 。<br>\n",
    "为什么 这 世上 会 有 人 以 刁难 他人 为乐 呢 ？<br>\n",
    "算了 算了 ， 我 看出来 了 ， 你们 都 想 看 男人 ！ 上 张 美 男图 。<br>\n",
    "偷拍 时 被 喵星 人 发现 了 。 ！ \\ 3 /<br>\n",
    "看看 你 的 名字 在 古代 是 什么 职业 ， 太 让 人 崩溃 了 。<br>\n",
    "居然 把 Windows Phone 8 写成 Win8 了 …… 要 严谨 啊<br>\n",
    "百合 真 香 啊 ！ 送给 回家 路上 的 人们 明天 后天 就 不行了 ！<br>\n",
    "刘翔 预算 摔倒 ！ 无缘 半决赛 ！ 主持人 哭了 。<br>\n",
    "谁 身边 没 几 个 能办 大 事 的 朋友 ？<br>\n",
    "一 位 妈妈 将 她 四 岁 儿子 的 涂鸦 做成 了 真实 的 毛绒 玩具<br>\n",
    "竟然 下雪 了 ， 不 喜欢 冬天 ， 天气 何时 才 变暖 啊 。<br>\n",
    "困扰 我 多 年 的 问题 终于 得 解了 。<br>\n",
    "大学生 一定 要 看 的 一 分钟 ， 它 能 让 你 奋斗 一辈子 alink<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 上面是30句样本文本，现在我们开始编写代码实现这个语言模型吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from numpy.random import shuffle\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "from visdom import Visdom\n",
    "from collections import Counter\n",
    "\n",
    "class deta_loader(object):\n",
    "    def __init__(self, N_gram=3,data_file=\"\"):\n",
    "        super(deta_loader,self).__init__()\n",
    "        \"\"\"\n",
    "        数据加载函数，实现文本数据的加载和处理\n",
    "        :param N_gram代表设置大小，即取前N_gram-1个词做为上下文\n",
    "        :param data_file,是传入的文本文件，要求每一行一个句子，然后用空格分好词\n",
    "        \"\"\"\n",
    "        self.N_gram=N_gram\n",
    "        self.data_file=data_file\n",
    "        self.words=[]\n",
    "        self.sents=[]\n",
    "        self.ids=[]\n",
    "        self.red_file()\n",
    "        self.word2idx=dict(zip(self.words,range(len(self.words))))\n",
    "        self.idx2word=dict(zip(range(len(self.words)),self.words))\n",
    "        self.sent2ids()\n",
    "        self.vocab_size=len(self.word2idx)\n",
    "        self.samples=[]\n",
    "        self.gen_ngram()\n",
    "        \n",
    "    def red_file(self):\n",
    "        \"\"\"\n",
    "        读文件函数，将文本文件读入数据\n",
    "        \"\"\"\n",
    "        with open(self.data_file) as f:\n",
    "            for line in f.readlines():\n",
    "                #对每一行分开空格为list\n",
    "                line=line.strip().split()\n",
    "                self.words.extend(line)\n",
    "                self.sents.append(line)\n",
    "                \n",
    "        self.all_words=self.words\n",
    "        self.words=list(set(self.words))\n",
    "                \n",
    "    def sent2ids(self):\n",
    "        \"\"\"\n",
    "        将文本数据转换为id数字化\n",
    "        \"\"\"\n",
    "        for sent in self.sents:\n",
    "            self.ids.append([self.word2idx[w] for w in sent])\n",
    "                \n",
    "    def gen_ngram(self):\n",
    "        \"\"\"\n",
    "        生成N-gram样本函数\n",
    "        \"\"\"\n",
    "        for sent in self.ids:\n",
    "            b = ngrams(sent,self.N_gram)\n",
    "            self.samples.extend([(list(item)[:self.N_gram-1],list(item)[-1]) for item in b])\n",
    "        shuffle(self.samples)\n",
    "        \n",
    "    def get_most_common(self,k):\n",
    "        c=Counter(self.all_words)\n",
    "        return c.most_common(k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class nlm(nn.Module):\n",
    "    \"\"\"语言模型网络类\"\"\"\n",
    "    def __init__(self, emb_dim, vocab_size):\n",
    "        super(nlm, self).__init__()\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.proj=nn.Linear(emb_dim, vocab_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        xw=self.word_emb(inp).sum(1)\n",
    "        out=self.proj(xw).exp()\n",
    "        self.softmax(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab713/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n",
      "/home/lab713/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-25761b782102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0minp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    viz = Visdom()\n",
    "    line = viz.line(np.arange(2))\n",
    "    \n",
    "    epochs=20\n",
    "    dl=deta_loader(data_file=\"weibo_data.txt\")\n",
    "    model=nlm(emb_dim=128,vocab_size=dl.vocab_size).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer=optim.Adam(model.parameters(),lr=0.0005)\n",
    "    \n",
    "    loss_list=[]\n",
    "    step_p=[]\n",
    "    loss_p=[]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for step,sample in  enumerate(dl.samples):\n",
    "            inp,tag=sample\n",
    "            inp=Variable(torch.from_numpy(np.array(inp))).view(1,-1).cuda()\n",
    "            tag=Variable(torch.LongTensor(torch.from_numpy(np.array([tag])))).cuda()\n",
    "            output=model(inp)\n",
    "            loss = criterion(output,tag)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            loss_list.append(loss.data[0])\n",
    "            # 剪裁参数梯度\n",
    "            optimizer.step()\n",
    "            #每1000步打印loss\n",
    "            if step%1000==0 :\n",
    "                loss_p.append(np.mean(loss_list))\n",
    "                step_p.append(step+epoch*len(dl.samples))\n",
    "                viz.line(\n",
    "                     X=np.array(step_p),\n",
    "                     Y=np.array(loss_p),\n",
    "                     win=line,\n",
    "                     opts=dict(legend=[\"Train_mean_loss\"]))\n",
    "                \n",
    "                loss_list=[]\n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('，', 7362), ('。', 6114), ('的', 5624), ('！', 3002), ('了', 2165), ('是', 1954), ('一', 1870), ('你', 1788), ('我', 1652), ('？', 1646), ('不', 1285), ('#', 1037), ('有', 1001), ('这', 937), ('个', 905), ('…', 876), ('在', 841), ('人', 748), ('转', 736), ('）', 700), ('（', 691), ('alink', 684), ('就', 654), ('：', 617), ('都', 605), ('【', 538), ('】', 538), ('会', 444), ('啊', 435), ('要', 421), ('好', 405), ('吗', 392), ('和', 383), ('能', 377), ('最', 356), ('很', 354), ('看', 347), ('什么', 337), ('这个', 333), ('也', 320), ('」', 317), ('「', 315), ('“', 311), ('大', 305), ('”', 304), ('太', 295), ('今天', 288), ('图', 284), ('想', 283), ('还', 277), ('过', 275), ('没', 267), ('吧', 266), ('我们', 260), ('吃', 252), ('自己', 251), ('来', 247), ('上', 246), ('说', 245), ('这样', 240), ('天', 237), ('中', 235), ('谁', 228), ('《', 227), ('》', 227), ('小', 224), ('去', 221), ('中国', 220), ('给', 219), ('喜欢', 215), ('大家', 214), ('、', 214), ('让', 209), ('北京', 208), ('——', 208), ('被', 208), ('种', 206), ('只', 203), ('爱', 200), ('没有', 197), ('又', 195), ('才', 194), (')', 192), ('(', 190), ('你们', 186), ('到', 185), ('年', 184), ('呢', 182), ('可以', 178), ('做', 177), ('多', 175), ('如果', 174), ('知道', 171), ('张', 169), ('请', 169), ('次', 166), ('把', 164), ('这么', 158), ('用', 151), ('得', 149), ('他', 149), ('三', 147), ('第一', 144), ('两', 144), ('看到', 140), ('世界', 138), ('就是', 134), ('\"', 133), ('再', 132), ('现在', 132), ('每', 132), ('＂', 131), ('怎么', 128), ('对', 127), ('图片', 126), ('里', 123), ('via', 122), ('据说', 121), ('还是', 121), ('啦', 120), ('生活', 120), ('几', 117), ('真', 116), ('与', 116), ('开始', 114), ('.', 114), ('下', 114), ('真的', 111), (':', 110), ('后', 109), ('觉得', 109), ('着', 108), ('点', 108), ('时候', 107), ('为', 106), ('那', 106), ('时', 103), ('事', 102), ('电影', 102), ('转发', 100), ('手机', 99), ('朋友', 98), ('老', 98), ('人生', 97), ('一下', 97), ('之', 96), ('照片', 96), ('美', 94), ('已经', 91), ('感觉', 90), ('走', 89), ('玩', 89), ('哪', 88), ('笑', 88), ('像', 88), ('而', 87), ('看看', 87), ('它', 86), ('叫', 86), ('但', 85), ('为什么', 84), ('句', 84), ('新', 83), ('比', 83), ('微', 83), ('见', 82), ('一起', 81), ('二', 81), ('一定', 79), ('各', 79), ('家', 79), ('来自', 78), ('其实', 78), ('因为', 78), ('早安', 78), ('位', 77), ('总', 77), ('-', 77), ('从', 76), ('男人', 76), ('快乐', 76), ('终于', 75), ('各位', 75), ('别', 75), ('博', 75), ('那些', 73), ('孩子', 73), ('睡', 72), ('前', 72), ('拍', 71), ('岁', 71), ('当', 70), ('跟', 70), ('告诉', 69), ('他们', 69), ('那么', 69), ('全', 68), ('话', 68), ('东西', 68), ('创意', 68), ('幸福', 67), ('买', 67), ('件', 67), ('明天', 66), ('却', 66), ('今晚', 66), ('更', 66), ('哦', 66), ('条', 66), ('别人', 66), ('最后', 65), ('原来', 65), ('出来', 64), ('多少', 63), ('晚安', 63), ('该', 63), ('网络', 63), ('四', 62), ('货', 62), ('所有', 60), ('发现', 60), ('这里', 60), ('日', 59), ('；', 59), ('猫', 59), ('她', 58), ('分钟', 58), ('部', 58), ('希望', 58), ('童鞋', 57), ('时间', 57), ('敢', 55), ('不要', 55), ('开心', 55), ('真是', 55), ('场', 55), ('一直', 55), ('日本', 55), ('喝', 54), ('那个', 54), ('本', 53), ('如何', 53), ('分', 53), ('广告', 53), ('早上', 53), ('应该', 53), ('公司', 53), ('手', 52), ('只有', 52), ('已', 52), ('还有', 52), ('发', 52), ('结束', 52), ('漫画', 51), ('妈妈', 51), ('地', 51), ('首', 51), ('只是', 51), ('高', 51), ('带', 51), ('一样', 51), ('生命', 51), ('呀', 50), ('求', 50), ('可', 50), ('很多', 50), ('打', 50), ('网友', 49), ('新闻', 49), ('比赛', 49), ('将', 48), ('设计', 48), ('真正', 48), ('上海', 47), ('哪个', 47), ('成', 47), ('字', 47), ('五', 47), ('生', 46), ('回', 46), ('狗', 46), ('记得', 46), ('为了', 46), ('小时', 46), ('早', 46), ('爱情', 46), ('超', 46), ('问题', 45), ('懂', 45), ('分享', 45), ('哪里', 45), ('需要', 45), ('居然', 44), ('可爱', 44), ('起', 44), ('刚', 44), ('心', 43), ('有时候', 43), ('最近', 43), ('准备', 43), ('这些', 43), ('永远', 43), ('晚上', 43), ('美国', 43), ('快', 43), ('开', 42), ('死', 42), ('10', 42), ('穿', 42), ('等', 42), ('感动', 41), ('2', 41), ('听', 41), ('小时候', 41), ('花', 41), ('必须', 41), ('女', 41), ('钱', 41), ('哥', 40), ('5', 40), ('十', 40), ('看图', 40), ('=', 40), ('向', 40), ('出', 40), ('工作', 40), ('牛', 40), ('心情', 40), ('版', 40), ('时光', 40), ('地方', 40), ('3', 39), ('月', 39), ('么', 39), ('感受', 39), ('微博', 39), ('某', 39), ('书', 39), ('空气', 39), ('如此', 39), ('简单', 39), ('关于', 39), ('重要', 39), ('起来', 38), ('谢谢', 38), ('iPhone', 38), ('游戏', 38), ('同学', 38), ('坚持', 38), ('事情', 38), ('一切', 38), ('香港', 38), ('伤', 38), ('视频', 37), ('神', 37), ('举手', 37), ('说明', 37), ('歌', 37), ('故事', 37), ('无', 37), ('您', 37), ('到底', 37), ('生日', 37), ('可能', 36), ('不错', 36), ('女人', 36), ('正在', 36), ('写', 36), ('想到', 36), ('越', 36), ('累', 36), ('比较', 36), ('样子', 35), ('话题', 35), ('老师', 35), ('住', 35), ('人类', 35), ('哈哈', 35), ('大学', 35), ('款', 35), ('>', 35), ('主人', 34), ('以后', 34), ('车', 34), ('相信', 34), ('祝', 34), ('蛋糕', 34), ('画', 34), ('8', 33), ('周', 33), ('突然', 33), ('央视', 33), ('秒', 33), ('以为', 33), ('长', 33), ('绝对', 33), ('容易', 33), ('回来', 33), ('过去', 32), ('天气', 32), ('回家', 32), ('干', 32), ('苹果', 32), ('木', 32), ('加油', 32), ('国家', 32), ('世界上', 32), ('睡觉', 32), ('区别', 32), ('瞬间', 32), ('学习', 32), ('度', 32), ('信', 31), ('以', 31), ('意思', 31), ('水', 31), ('今日', 31), ('枪', 31), ('女生', 31), ('周末', 31), ('滴', 31), ('帅', 31), ('有些', 31), ('有点', 31), ('连', 31), ('夏天', 31), ('1', 31), ('愿', 31), ('同', 31), ('坐', 30), ('受', 30), ('成功', 30), ('国', 30), ('未', 30), ('人民', 30), ('怕', 30), ('今年', 30), ('先', 30), ('城市', 30), ('问', 30), ('非常', 29), ('/', 29), ('嘛', 29), ('看完', 29), ('声音', 29), ('然后', 29), ('努力', 29), ('少', 29), ('神马', 29), ('改变', 29), ('路', 29), ('难', 29), ('球', 29), ('送', 29), ('下午', 29), ('吃饭', 29), ('此', 29), ('身边', 28), ('之后', 28), ('半', 28), ('适合', 28), ('号', 28), ('无法', 28), ('值得', 28), ('放弃', 28), ('时代', 28), ('表情', 28), ('证明', 28), ('猜', 28), ('感谢', 28), ('起床', 28), ('找', 28), ('搞', 28), ('行', 28), ('未来', 27), ('靠', 27), ('消息', 27), ('2012', 27), ('一点', 27), ('当年', 27), ('霸气', 27), ('衣服', 27), ('历史', 27), ('听说', 27), ('代', 27), ('广州', 27), ('出现', 27), ('阳光', 27), ('imgur', 27), ('总是', 27), ('震惊', 27), ('美好', 27), ('路上', 26), ('喵星人', 26), ('NBA', 26), ('遇到', 26), ('刚刚', 26), ('旅行', 26), ('曾经', 26), ('所以', 26)]\n"
     ]
    }
   ],
   "source": [
    "print(dl.get_most_common(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vec(w,embedding=model.word_emb,word2idx=dl.word2idx):\n",
    "    w= torch.tensor([word2idx[w]]).cuda()\n",
    "    w_vec=embedding(w).data.cpu().numpy()\n",
    "    return w_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.4721228e-01  4.8448700e-01  1.5144777e+00  9.5147753e-01\n",
      "   1.1991516e+00  2.3802574e-01 -3.2100305e-01 -1.3965905e-01\n",
      "  -5.8980620e-01 -1.3014680e+00  1.2092247e+00  3.9012423e-01\n",
      "  -2.0070937e-01 -6.4108288e-01  1.0200247e+00 -1.3535341e+00\n",
      "  -1.4325051e+00  2.9552910e-01 -1.2998731e-01  8.7058049e-01\n",
      "  -8.0749679e-01  1.1529236e+00  1.2957534e+00 -9.7147238e-01\n",
      "   1.2506478e+00 -1.7023355e+00 -1.2845010e+00  4.6664897e-01\n",
      "  -2.7361210e-02  1.5077260e+00 -1.0384908e+00  6.1781924e-02\n",
      "   2.9631966e-01 -2.1747442e-01  8.3622068e-02  1.6053553e-01\n",
      "   3.5777304e-01 -9.4086355e-01  8.7050891e-01  2.2529464e+00\n",
      "   4.8131478e-01 -1.6052644e-01 -1.2991546e+00 -7.6134598e-01\n",
      "   2.8889695e-01  1.1788763e+00  4.1251451e-01 -1.5097270e+00\n",
      "   2.0492241e-02 -5.1674538e-04 -2.5935450e-01  9.6186481e-02\n",
      "   1.0805404e+00  5.0584618e-02  8.3632082e-01 -4.2637479e-02\n",
      "   1.4517438e+00 -1.8101935e+00 -2.9617688e-01  1.3959101e+00\n",
      "  -1.2961789e+00 -1.6156006e+00  4.0722337e-01  1.6145271e-01\n",
      "  -2.0146232e+00 -1.7644478e-01  7.1603042e-01 -2.0211458e+00\n",
      "   5.0062165e-02 -1.0736527e+00  5.1187909e-01  6.4528072e-01\n",
      "   9.7725236e-01 -2.8830576e-01  5.3274876e-01 -1.0106019e+00\n",
      "   2.3548765e-01 -1.2410505e+00  1.8892785e+00  5.9373677e-01\n",
      "  -1.6900400e+00 -7.1500875e-02 -1.4694453e+00  2.3627564e-01\n",
      "  -3.3930737e-01  9.9212486e-01 -5.8281732e-01  3.3665004e-01\n",
      "   4.5234767e-01  5.5253392e-01  7.0276368e-01  7.9841775e-01\n",
      "   1.0466300e-01  1.7268424e+00  2.1054568e+00 -1.8856947e+00\n",
      "  -6.0591325e-03 -6.3712068e-02 -6.8903142e-01 -7.6863110e-01\n",
      "  -4.6803197e-01 -7.5171399e-01 -5.0482309e-01  3.5734195e-01\n",
      "  -1.2101463e-01  1.1437153e+00  9.8765171e-01 -1.1772413e+00\n",
      "   4.3447316e-01  7.8105442e-02 -1.2119976e+00  1.4487309e+00\n",
      "   1.7724288e-01 -4.6223905e-02  1.8863355e-01 -7.5486708e-01\n",
      "   3.2457128e-01  9.0790421e-01 -4.3896642e-01  6.6359699e-01\n",
      "  -3.5074160e-01  7.8610945e-01 -2.7220566e-02  1.0194542e+00\n",
      "   1.4437286e+00  9.9562746e-01  7.0090353e-01 -6.8067884e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(get_vec(\"时间\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim(w1,w2,embedding=model.word_emb,word2idx=dl.word2idx):\n",
    "    \"\"\"\n",
    "    利用训练好的模型计算cos相似度\n",
    "    \"\"\"\n",
    "    w1= torch.tensor([word2idx[w1]]).cuda()\n",
    "    w2= torch.tensor([word2idx[w2]]).cuda()\n",
    "    w1_vec=embedding(w1).data.cpu().numpy()\n",
    "    w2_vec=embedding(w2).data.cpu().numpy()\n",
    "    \n",
    "    num = float(np.dot(w1_vec,w2_vec.T)) \n",
    "    denom = np.linalg.norm(w1_vec) * np.linalg.norm(w2_vec)\n",
    "    cos = num / denom #余弦值\n",
    "    sim = 0.5 + 0.5 * cos #归一化   \n",
    "    print(sim)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4606666786294646\n",
      "0.5187198135575075\n",
      "0.5678678651780635\n",
      "0.595745311236103\n"
     ]
    }
   ],
   "source": [
    "sim('美国','非常')\n",
    "sim('美国','香港')\n",
    "sim('美国','北京')\n",
    "sim('美国','日本')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
